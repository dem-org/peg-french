# Paramètres LLM

Lorsque vous travaillez avec des prompts, vous interagissez avec le LLM soit via une API soit directement. Vous pouvez configurer certains paramètres pour obtenir des résultats différents. 

**Température** - Plus la température est basse, plus les résultats sont déterministes, dans le sens où le token suivant qui sera choisi sera toujours celui qui est le plus probable. En augmentant la température, on peut introduire plus d'aléatoire et encourager des résultats plus diversifiés ou créatifs. On augmente en réalité l'importance des autres tokens possibles. Dans le cadre d'applications, on pourrait souhaiter utiliser une valeur de température plus faible pour des tâches comme les questions-réponses axées sur des faits, de manière à favoriser des réponses plus précises et concises. Pour la création de poèmes ou d'autres tâches créatives, il peut être bénéfique d'augmenter la valeur de la température.

**Top_p** - De même, avec `top_p`, une technique d'échantillonnage avec température appelée échantillonnage de noyau, vous pouvez contrôler le degré de déterminisme avec lequel le modèle génère une réponse. Si vous recherchez des réponses exactes et factuelles, gardez cette valeur basse. Si vous recherchez des réponses plus diversifiées, augmentez la valeur. 

La recommandation générale est de modifier l'un des deux, pas les deux.

Avant de commencer avec quelques exemples de base, gardez à l'esprit que vos résultats peuvent varier en fonction de la version de LLM que vous utilisez.
