# Zero-Shot Prompting
Les LLMs aujourd'hui entraînés sur de grandes quantités de données et réglés pour suivre des instructions, sont capables d'effectuer des tâches zero-shot. Nous avons essayé quelques exemples de zero-shot dans la section précédente. Voici l'un des exemples que nous avons utilisés :

*Prompt:*
```
Classez le texte en neutre, négatif ou positif. 

Texte : Je pense que les vacances sont bonnes.
Sentiment :
```

*Output:*
```
Neutre
```

Notez que dans le prompt ci-dessus, nous n'avons pas fourni d'exemples au modèle - c'est la capacité zero-shot qui est à l'œuvre. 

Il a été démontré que l'instruction de tuning (TODO) améliorait l'apprentissage zero-shot [Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). L'instruction de tuning (TODO) est essentiellement le concept de finetuning (TODO) des modèles sur des ensembles de données décrits par des instructions. En outre, [RLHF](https://arxiv.org/abs/1706.03741) (reinforcement learning from human feedback) a été adopté pour mettre à l'échelle l'instruction de tuning (TODO), le modèle étant aligné pour mieux s'adapter aux préférences humaines. Ce développement récent alimente des modèles tels que ChatGPT. Nous examinerons toutes ces approches et méthodes dans les sections suivantes.

Lorsque le zero-shot ne fonctionne pas, il est recommandé de fournir des démonstrations ou des exemples dans le prompt, ce qui conduit au few-shot prompting. Dans la section suivante, nous démontrons le few-shot prompting.
