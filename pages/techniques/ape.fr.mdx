# Automatic Prompt Engineer (APE)

import { Callout, FileTree } from 'nextra-theme-docs'
import {Screenshot} from 'components/screenshot'
import APE from '../../img/APE.png'
import APECOT from '../../img/ape-zero-shot-cot.png'

<Screenshot src={APE} alt="APE" />
Image Source: [Zhou et al., (2022)](https://arxiv.org/abs/2211.01910)


[Zhou et al., (2022)](https://arxiv.org/abs/2211.01910) propose l'automatic prompt engineer (APE) un cadre pour la génération et la sélection automatiques d'instructions. Le problème de la génération d'instructions est présenté comme une synthèse du langage naturel abordée comme un problème d'optimisation en boîte noire utilisant des LLMs pour générer et rechercher des solutions candidates. 

La première étape implique un LLM (en tant que modèle d'inférence) qui reçoit des démonstrations de sortie pour générer des instructions candidates pour une tâche. Ces solutions candidates guideront la procédure de recherche. Les instructions sont exécutées à l'aide d'un modèle cible, puis l'instruction la plus appropriée est sélectionnée sur la base des scores d'évaluation calculés. 

APE découvre un meilleur zero-shot CoT prompt que celui élaboré par le prompt engineering humain "Réfléchissons étape par étape" ([Kojima et al., 2022](https://arxiv.org/abs/2205.11916)).

Le prompt "Travaillons étape par étape pour être sûrs d'avoir la bonne réponse." suscite un raisonnement chain-of-though et améliore les performances sur les benchmarks MultiArith et GSM8K :


<Screenshot src={APECOT} alt="APECOT" />
Image Source: [Zhou et al., (2022)](https://arxiv.org/abs/2211.01910)

Ce document aborde un sujet important lié au prompt engineering, à savoir l'idée d'optimiser automatiquement les prompts. Bien que nous n'approfondissions pas ce sujet dans ce guide, voici quelques articles clés si vous êtes intéressé par le sujet :

- [AutoPrompt](https://arxiv.org/abs/2010.15980) - propose une approche pour créer automatiquement des messages-guides pour un ensemble varié de tâches en se basant sur la recherche guidée par gradient.
- [Prefix Tuning](https://arxiv.org/abs/2101.00190) - une alternative légère au réglage fin qui ajoute un préfixe continu formable pour les tâches NLG. 
- [Prompt Tuning](https://arxiv.org/abs/2104.08691) - propose un mécanisme d'apprentissage de prompts doux par rétropropagation.
